{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPe+j1Rly+/6Pd1O99elLYG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Automatic Diffrenciation Tool :  Autograd\n","\n","Autograd is core component of pytorch that provides automatic differenciation for tenso.\n","\n","It enables gradient computation, which is essential for training of Ml models using optimization algo like GD"],"metadata":{"id":"qicjaJuF4Xvj"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"2XdZzjdr4S-g","executionInfo":{"status":"ok","timestamp":1736524585644,"user_tz":-330,"elapsed":5019,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}}},"outputs":[],"source":["import torch"]},{"cell_type":"code","source":["x = torch.tensor(3.0 ,requires_grad=True)\n","y = x**2"],"metadata":{"id":"XF3_yWxx7fNl","executionInfo":{"status":"ok","timestamp":1736524612112,"user_tz":-330,"elapsed":571,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_EB4FUS7rzL","executionInfo":{"status":"ok","timestamp":1736524622385,"user_tz":-330,"elapsed":511,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}},"outputId":"5f1d1280-eb75-4851-f4eb-5d2169a48fbe"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3., requires_grad=True)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZI_LB-rC7tMK","executionInfo":{"status":"ok","timestamp":1736524627637,"user_tz":-330,"elapsed":497,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}},"outputId":"4164f320-767c-4628-9012-5966cc4e0218"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(9., grad_fn=<PowBackward0>)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["y.backward()"],"metadata":{"id":"2s5gKGR-7vbD","executionInfo":{"status":"ok","timestamp":1736524711430,"user_tz":-330,"elapsed":584,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["x.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvRaQFN58D3e","executionInfo":{"status":"ok","timestamp":1736524728293,"user_tz":-330,"elapsed":5,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}},"outputId":"8df9950b-6ee7-4ac0-eebe-9def0850d8cb"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(6.)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["# Example 02"],"metadata":{"id":"nN4pFogY8QCW"}},{"cell_type":"code","source":["x =  torch.tensor(3.0 ,requires_grad=True)\n","y = x**2\n","z  =  torch.sin(y)\n","z.backward()\n","x.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kch9SGx8H-r","executionInfo":{"status":"ok","timestamp":1736524929632,"user_tz":-330,"elapsed":551,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}},"outputId":"b2d94e3f-83ac-45bd-dc08-6f8a4f43b4a4"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-5.4668)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Example 03 Neural Network\n","\n","\n","\n"],"metadata":{"id":"hDqEdAr99Vgn"}},{"cell_type":"code","source":["import torch\n","\n","# Generate sample data\n","x = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Inputs (1D tensor)\n","y = torch.tensor([2.0, 4.0, 6.0, 8.0])  # Targets (1D tensor)\n","\n","# Initialize weights and bias\n","w = torch.tensor([0.0], requires_grad=True)  # Weight (vector)\n","b = torch.tensor([0.0], requires_grad=True)  # Bias (scalar)\n","\n","# Learning rate\n","lr = 0.01\n","\n","# Number of epochs\n","epochs = 1000\n","\n","# Training loop\n","for epoch in range(epochs):\n","    # Forward pass\n","    y_pred = x * w + b\n","\n","    # Compute loss (Mean Squared Error)\n","    loss = torch.mean((y_pred - y) ** 2)\n","\n","    # Backward pass (compute gradients)\n","    loss.backward()\n","\n","    # Update parameters manually\n","    with torch.no_grad():\n","        w -= lr * w.grad\n","        b -= lr * b.grad\n","\n","        # Zero the gradients (clearing gradient)\n","        w.grad.zero_()\n","        b.grad.zero_()\n","\n","    # Print loss every 100 epochs\n","    if (epoch + 1) % 100 == 0:\n","        print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n","\n","# Final weights and bias\n","print(f\"Trained weight: {w.tolist()}, Trained bias: {b.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1Yas8_m85VH","executionInfo":{"status":"ok","timestamp":1736525692918,"user_tz":-330,"elapsed":510,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}},"outputId":"855d46b2-5e79-47c0-b243-d9220cb31f81"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 100, Loss: 0.0343\n","Epoch 200, Loss: 0.0188\n","Epoch 300, Loss: 0.0103\n","Epoch 400, Loss: 0.0057\n","Epoch 500, Loss: 0.0031\n","Epoch 600, Loss: 0.0017\n","Epoch 700, Loss: 0.0009\n","Epoch 800, Loss: 0.0005\n","Epoch 900, Loss: 0.0003\n","Epoch 1000, Loss: 0.0002\n","Trained weight: [1.9896587133407593], Trained bias: 0.0304\n"]}]},{"cell_type":"markdown","source":["# Disable Gradient Tracking"],"metadata":{"id":"io2xyRKeA6kj"}},{"cell_type":"code","source":["w.requires_grad_(False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4p0MbXbd_zhL","executionInfo":{"status":"ok","timestamp":1736526018515,"user_tz":-330,"elapsed":576,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}},"outputId":"4b72a280-22c4-4f4f-c3d3-7c67ac629fe9"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1.9897])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["#due to grqadient tracking is off\n","w.backward()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"3XnlSNesBDK-","executionInfo":{"status":"error","timestamp":1736526040148,"user_tz":-330,"elapsed":511,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}},"outputId":"5fbc925c-5e29-4e7c-e1cc-9b2533625f27"},"execution_count":11,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-fdaeb047dc58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"]}]},{"cell_type":"code","source":["# Disable Autograd\n","X = torch.tensor(4.0 , requires_grad=True)\n","with torch.no_grad():\n","  Y = X**2\n","  Y.backward()\n","  X.grad\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"lXisArgfBITx","executionInfo":{"status":"error","timestamp":1736526225213,"user_tz":-330,"elapsed":470,"user":{"displayName":"Robert downey","userId":"07427735734089550956"}},"outputId":"71c8078a-e84e-4ddc-fd7e-1cacdd1d8ed1"},"execution_count":12,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-3ed3019ce154>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"]}]}]}